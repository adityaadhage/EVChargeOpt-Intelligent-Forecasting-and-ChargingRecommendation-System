# -*- coding: utf-8 -*-
"""Yet another copy of EV_Project_Origi_GB3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IRxLpYfFjURLtTikduKvm47hJi9w2Tn8
"""

import pandas as pd
import numpy as np

df = pd.read_csv("/content/part-00000-9b8b5fff-8a6f-4b5d-b76f-202824892321-c000.csv")
df.head()

# Check for missing values
print(df.isnull().sum())

# Drop rows or fill if required
df.dropna(inplace=True)  # or df.fillna(0)

num_cols = df.select_dtypes(include=['int64', 'float64']).columns
print(num_cols)

df.loc[:, num_cols] = (df.loc[:, num_cols] - df.loc[:, num_cols].mean()) / df.loc[:, num_cols].std()

df.loc[:, num_cols].std()

df.loc[:, num_cols].mean()

# Detect outliers using IQR
for col in num_cols:
    q1, q3 = df[col].quantile([0.25, 0.75])
    IQR = q3 - q1
    lower_bound = q1 - 1.5 * IQR
    upper_bound = q3 + 1.5 * IQR
    count = df[(df[col] < lower_bound) | (df[col] > upper_bound)].shape[0]
    print(f"{col}: {count} outliers")

for col in num_cols:
    q1, q3 = df[col].quantile([0.25, 0.75])
    IQR = q3 - q1
    lower_bound = q1 - 1.5 * IQR
    upper_bound = q3 + 1.5 * IQR
    df.loc[df[col] < lower_bound, col] = lower_bound
    df.loc[df[col] > upper_bound, col] = upper_bound

df = pd.get_dummies(df, columns=["Vehicle_Types", "Charging_Preferences", "EV_Usage_Patterns"], drop_first=True)

df.info()

import matplotlib.pyplot as plt
import seaborn as sns
# Calculate the correlation matrix
corr = df.corr(numeric_only=True)

# Plot the heatmap
plt.figure(figsize=(12, 10))
sns.heatmap(corr, annot=False, cmap='coolwarm')
plt.title('Correlation Matrix')
plt.show()

# Display correlations with the target variable
print(corr['Charging_Load_kW'].sort_values(ascending=False))

import seaborn as sns
print(df['Charging_Load_kW'].describe())
sns.histplot(df['Charging_Load_kW'], kde=True)

# Count negative values
neg_count = (df["Charging_Load_kW"] < 0).sum()
print(f"âš ï¸ Negative values: {neg_count} / {len(df)}")

df["Charging_Load_kW"] = df["Charging_Load_kW"].clip(lower=0)

df = df[df["Charging_Load_kW"] >= 0]

df["Charging_Load_kW"] = np.log1p(df["Charging_Load_kW"])  # log(1 + x)

# target = "Charging_Load_kW"
# X = df.drop(columns=[target, 'Timestamp'], errors='ignore')  # Exclude timestamp if present
# y = df[target]

corr = df.corr(numeric_only=True)
print(corr['Charging_Load_kW'].sort_values(ascending=False))

X = df.drop(columns=["Charging_Load_kW", "Timestamp", "Load_per_EV"], errors='ignore')
y = df["Charging_Load_kW"]

print(df[['Holiday_Indicator', 'Precipitation_mm', 'Incentives_Programs']].isnull().sum())

df.drop(columns=["Incentives_Programs", "Holiday_Indicator", "Precipitation_mm"], inplace=True, errors="ignore")

df["Load_per_EV"] = df["Charging_Load_kW"] / df["Fleet_Size"]
df["Power_per_Station"] = df["Charging_Power_Rating_kW"] / df["Number_of_Charging_Stations"]

df["HourGroup"] = pd.cut(df["Hour"], bins=[0,6,12,18,24], labels=["Night", "Morning", "Afternoon", "Evening"])

# nan_counts = df.isnull().sum()
# print(nan_counts[nan_counts > 0])

# Fill NaNs in numeric columns only
numeric_cols = df.select_dtypes(include=["number"]).columns
df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].mean())

from sklearn.feature_selection import mutual_info_regression
# Make sure no NaNs in X and y
X = df.drop("Charging_Load_kW", axis=1)
y = df["Charging_Load_kW"]
X = X.select_dtypes(include=["number"])  # or after encoding all categoricals
mi = mutual_info_regression(X, y)
mi_scores = pd.Series(mi, index=X.columns).sort_values(ascending=False)
print(mi_scores)

df['Timestamp'] = pd.to_datetime(df['Timestamp'])

# Extract useful time components
df['Hour'] = df['Timestamp'].dt.hour
df['Day'] = df['Timestamp'].dt.day
df['Month'] = df['Timestamp'].dt.month
df['DayOfWeek'] = df['Timestamp'].dt.dayofweek
df['IsWeekend'] = df['DayOfWeek'].isin([5, 6]).astype(int)
df['IsPeakHour'] = df['Hour'].isin([7, 8, 17, 18, 19]).astype(int)

df['Load_per_EV'] = df['Charging_Load_kW'] / df['Fleet_Size']
df['Power_per_Station'] = df['Charging_Power_Rating_kW'] / (df['Number_of_Charging_Stations'] + 1e-6)
df['Distance_per_Charge'] = df['Total_Distance_Driven_km'] / (df['Charging_Duration_hours'] + 1e-3)

# Create a comfort index (example of non-linear interaction)
df['Temp_Humidity_Index'] = df['Temperature_C'] * df['Humidity_%']
df['Extreme_Weather'] = ((df['Temperature_C'] < 5) | (df['Temperature_C'] > 35)).astype(int)

df['Cost_per_kWh'] = df['Electricity_Prices_USD'] / (df['Charging_Efficiency'] + 1e-6)

df = df.sort_values('Timestamp')
df['Prev_Load'] = df['Charging_Load_kW'].shift(1)
df['Prev_Load_Change'] = df['Charging_Load_kW'] - df['Prev_Load']

df.dropna(inplace=True)  # or impute with df.fillna()

non_numeric_cols = X.select_dtypes(include=['object']).columns
print("Non-numeric columns:", non_numeric_cols.tolist())

# STEP 1: Drop datetime and ID-like columns if not engineered
columns_to_drop = ['Timestamp']  # add others if needed
X = X.drop(columns=[col for col in columns_to_drop if col in X.columns], errors='ignore')

# STEP 2: Convert object/categorical columns using one-hot encoding
X = pd.get_dummies(X, drop_first=True)

# STEP 3: Ensure all remaining columns are numeric
X = X.apply(pd.to_numeric, errors='coerce')

# STEP 4: Fill any resulting NaNs
X.fillna(X.mean(), inplace=True)

# Now X is clean â†’ You can safely apply mutual_info_regression or any ML model
from sklearn.feature_selection import mutual_info_regression

mi_scores = mutual_info_regression(X, y)
mi_series = pd.Series(mi_scores, index=X.columns).sort_values(ascending=False)
print(mi_series)

X = pd.get_dummies(X, drop_first=True)  # safe for most ML models

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# pip install -U scikit-learn

from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_squared_error, r2_score

# Gradient Boosting
gb_model = GradientBoostingRegressor(random_state=42)
gb_model.fit(X_train_scaled, y_train)
y_pred_log = gb_model.predict(X_test_scaled)
# Reverse log
y_pred = np.expm1(y_pred_log)
y_test_orig = np.expm1(y_test)

# Metrics
from sklearn.metrics import mean_squared_error, r2_score
rmse = np.sqrt(mean_squared_error(y_test_orig, y_pred))
r2 = r2_score(y_test_orig, y_pred)
print(f"âœ… RMSE: {rmse:.2f}")
print(f"ðŸ“ˆ RÂ² Score: {r2:.4f}")

from sklearn.model_selection import cross_val_score

scores = cross_val_score(GradientBoostingRegressor(random_state=42), X, y, scoring='r2', cv=5)
print("Cross-validated RÂ² scores:", scores)
print("Average RÂ²:", np.mean(scores))

# from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
# from xgboost import XGBRegressor
# from lightgbm import LGBMRegressor
# from sklearn.model_selection import cross_val_score, KFold
# from sklearn.metrics import make_scorer, mean_squared_error, r2_score
# import numpy as np

# # Reverse the log before scoring
# def rmse(y_true, y_pred):
#     return np.sqrt(mean_squared_error(np.expm1(y_true), np.expm1(y_pred)))

# rmse_scorer = make_scorer(rmse, greater_is_better=False)

# models = {
#     "Random Forest": RandomForestRegressor(random_state=42),
#     "Gradient Boosting": GradientBoostingRegressor(random_state=42),
#     "XGBoost": XGBRegressor(random_state=42),
#     "LightGBM": LGBMRegressor(random_state=42)
# }

# cv = KFold(n_splits=5, shuffle=True, random_state=42)

# results = {}

# for name, model in models.items():
#     scores = cross_val_score(model, X, y, scoring=rmse_scorer, cv=cv)
#     r2_scores = cross_val_score(model, X, y, scoring='r2', cv=cv)

#     results[name] = {
#         "RMSE (mean)": -np.mean(scores),
#         "RMSE (std)": np.std(scores),
#         "RÂ² (mean)": np.mean(r2_scores),
#         "RÂ² (std)": np.std(r2_scores)
#     }

# # Print comparison table
# import pandas as pd
# df_results = pd.DataFrame(results).T.sort_values(by="RMSE (mean)")
# display(df_results)

import matplotlib.pyplot as plt
import seaborn as sns

feature_imp = pd.Series(gb_model.feature_importances_, index=X.columns).sort_values(ascending=False)

plt.figure(figsize=(10,6))
sns.barplot(x=feature_imp, y=feature_imp.index)
plt.title("ðŸ” Feature Importance")
plt.xlabel("Importance Score")
plt.ylabel("Feature Name")
plt.tight_layout()
plt.show()

import joblib

joblib.dump(gb_model, "ev_load_forecast_model_GB.pkl")

gb_model1 = joblib.load("ev_load_forecast_model_GB.pkl")
new_pred = gb_model.predict(X_test[:10])
print("ðŸ”® Future Prediction:", new_pred)

X_test_copy = X_test[:10].copy()
X_test_copy["Predicted_Load_kW"] = new_pred
X_test_copy.reset_index(drop=True, inplace=True)
X_test_copy

#Compaire Actual vs Predicted Load
comparison_df = pd.DataFrame({
    "Actual Load (kW)": y_test[:10].values,
    "Predicted Load (kW)": new_pred
})
print(comparison_df)

plt.figure(figsize=(8,5))
plt.plot(y_test[:20].values, label='Actual', marker='o')
plt.plot(gb_model.predict(X_test[:20]), label='Predicted', marker='x')
plt.title("Actual vs Predicted Charging Load")
plt.xlabel("Sample Index")
plt.ylabel("Charging Load (kW)")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

def prepare_future_features(last_timestamp, hours=24, static_features=None):
    """
    Prepare a dataframe with 21 features needed for prediction.
    static_features: dict containing static or constant feature values for features that
                     cannot be derived from timestamp.
    """
    # Create future timestamps
    future_hours = pd.date_range(start=last_timestamp + pd.Timedelta(hours=1), periods=hours, freq='H')
    df = pd.DataFrame({'Timestamp': future_hours})

    # Time-based features
    df['Day_of_Week'] = df['Timestamp'].dt.dayofweek
    df['Hour'] = df['Timestamp'].dt.hour
    df['DayOfWeek'] = df['Timestamp'].dt.dayofweek  # duplicated but model expects both
    df['Month'] = df['Timestamp'].dt.month
    df['IsWeekend'] = df['Day_of_Week'].isin([5,6]).astype(int)

    # Fill static or constant features
    # If static_features not provided, use placeholders or zeros (replace as needed)
    defaults = {
        'Fleet_Size': 100,
        'Average_Battery_Capacity_kWh': 40,
        'Number_of_Charging_Stations': 10,
        'Charging_Power_Rating_kW': 7.4,
        'Charging_Efficiency': 0.9,
        'Total_Distance_Driven_km': 50,
        'Average_Speed_kmh': 40,
        'Loading_Unloading_Times_hours': 1,
        'Temperature_C': 25,
        'Humidity_%': 60,
        'Previous_Charging_Loads_kW': 5,
        'Charging_Duration_hours': 1,
        'Electricity_Prices_USD': 0.12,
        'Grid_Demand_MW': 500,
        'Load_per_EV': 5,
        'Power_per_Station': 3
    }
    if static_features is not None:
        defaults.update(static_features)

    # Assign static features to the dataframe
    for feat, val in defaults.items():
        df[feat] = val

    # Order columns exactly as model expects:
    feature_cols = ['Day_of_Week', 'Fleet_Size', 'Average_Battery_Capacity_kWh', 'Number_of_Charging_Stations',
                    'Charging_Power_Rating_kW', 'Charging_Efficiency', 'Total_Distance_Driven_km', 'Average_Speed_kmh',
                    'Loading_Unloading_Times_hours', 'Temperature_C', 'Humidity_%', 'Previous_Charging_Loads_kW',
                    'Charging_Duration_hours', 'Electricity_Prices_USD', 'Grid_Demand_MW', 'Hour', 'DayOfWeek',
                    'Month', 'IsWeekend', 'Load_per_EV', 'Power_per_Station']

    return df[feature_cols + ['Timestamp']]  # keep Timestamp at the end for reference

# Example usage:
last_timestamp = pd.Timestamp('2025-08-09 12:00:00')

# Optional: provide any updated static features you want to override
static_values = {
    'Fleet_Size': 120,
    'Temperature_C': 30,
    'Grid_Demand_MW': 550
}

future_df = prepare_future_features(last_timestamp, hours=24, static_features=static_values)

print(future_df.head())

import joblib

model = joblib.load('/content/ev_load_forecast_model_GB.pkl')  # load your trained model

X_future = future_df.drop(columns=['Timestamp'])  # only features

future_df['Predicted_Load_kW'] = model.predict(X_future)

print(future_df[['Timestamp', 'Predicted_Load_kW']])

start_hour = int(input("Start hour (0-23): "))
end_hour = int(input("End hour (0-23): "))

filtered = future_df[(future_df['Hour'] >= start_hour) & (future_df['Hour'] <= end_hour)]
recommended = filtered.nsmallest(3, 'Predicted_Load_kW')

print(f"Best charging times between {start_hour}:00 and {end_hour}:00:")
for _, row in recommended.iterrows():
    print(f"{row['Timestamp']} with predicted load {row['Predicted_Load_kW']:.2f} kW")